# (Mostly arXiv) papers I read in 2017.

- [ ] [Self-critical Sequence Training for Image Captioning](https://arxiv.org/pdf/1612.00563v1.pdf)
- [ ] [Continuous multilinguality with language vectors](https://arxiv.org/abs/1612.07486)
- [ ] [Learning from Simulated and Unsupervised Images through Adversarial Training](https://arxiv.org/abs/1612.07512)
- [ ] [How to Train Your Deep Neural Network with Dictionary Learning](https://arxiv.org/abs/1612.07454)
- [ ] [Quantum Machine Learning without Measurements](https://arxiv.org/abs/1612.05535)
- [ ] [Online Sequence-to-Sequence Active Learning for Open-Domain Dialogue Generation](https://arxiv.org/abs/1612.03929)
- [ ] [Learning representations through stochastic gradient descent in cross-validation error](https://arxiv.org/abs/1612.02879)
- [ ] [Probabilistic Neural Programs](https://arxiv.org/abs/1612.00712)
- [ ] [Neural Document Embeddings for Intensive Care Patient Mortality Prediction](https://arxiv.org/abs/1612.00467)
- [ ] [Transfer Learning Across Patient Variations with Hidden Parameter Markov Decision Processes](https://arxiv.org/abs/1612.00475)
- [ ] [Semantic Compositional Networks for Visual Captioning](https://arxiv.org/abs/1611.08002)
- [ ] [Emergent Logical Structure in Vector Representations of Neural Readers](https://arxiv.org/abs/1611.07954)
- [ ] [Can Active Memory Replace Attention?](https://arxiv.org/abs/1610.08613)
- [ ] [The Predictron: End-To-End Learning and Planning](https://arxiv.org/abs/1612.08810)
- [ ] [Feedback Networks](https://arxiv.org/abs/1612.09508)
- [ ] [NIPS 2016 Tutorial: Generative Adversarial Networks](https://arxiv.org/abs/1701.00160)
- [ ] [Meta-Unsupervised-Learning: A supervised approach to unsupervised learning](https://arxiv.org/abs/1612.09030)
- [ ] [A Learned Representation For Artistic Style](https://arxiv.org/abs/1610.07629)
- [ ] [Revisiting Batch Normalization For Practical Domain Adaptation](https://arxiv.org/abs/1603.04779)
